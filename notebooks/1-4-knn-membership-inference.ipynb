{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75916e17",
   "metadata": {},
   "source": [
    "# K-Nearest-Neighbors Membership Inference Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6661d3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.ensemble as es\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e1de41-a5ee-4520-8060-00873915a3a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threads = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f17097ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:xai-privacy:Loading dataset 1: heart disease (numeric features) ...\n",
      "INFO:xai-privacy:Loading dataset 2: census income (categorical features) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Age: removed 0 rows for missing values.\n",
      "Feature RestingBP: removed 59 rows for missing values.\n",
      "Feature Cholesterol: removed 27 rows for missing values.\n",
      "Feature FastingBS: add unknown category 2.0\n",
      "Feature RestingECG: add unknown category 3.0\n",
      "Feature MaxHR: removed 0 rows for missing values.\n",
      "Feature Oldpeak: removed 7 rows for missing values.\n",
      "Feature ST_Slope: add unknown category 4.0\n",
      "Feature CA: add unknown category 4.0\n",
      "Feature Thal: add unknown category 8.0\n",
      "Dropped 271 of 1097\n",
      "Dropped 273 of 1097\n",
      "Dropped 277 of 1097\n",
      "Dropped: 2399 of 32561\n",
      "census: Dropped 3848 of 30162\n",
      "num: Dropped 19859 of 30162\n",
      "cat: Dropped 12136 of 30162\n"
     ]
    }
   ],
   "source": [
    "%run experiment_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b77aa19b-a2fb-4a9b-95ae-0b44e584390e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('xai-privacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a61bbc",
   "metadata": {},
   "source": [
    "This notebook will go through the experiment for membership inference with KNN as an explanation. Membership inference means an attacker with access to the explanation can tell for a sample whether it was included in the training data or not.\n",
    "\n",
    "The idea for KNN membership inference is as follows: Enter the given sample and check whether the sample is returned as one of its own nearest neighbors. If it is, it is part of the training data. Otherwise it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488dc98b",
   "metadata": {},
   "source": [
    "First, we have to create our own wrapper class for the default scikit-learn KNN classifier. This is necessary because the scikit-learn implementation only returns the indices of the nearest neighbors. However, in order to be useful explanation to a user, the actual feature values of the nearest neighbors need to be returned. This is done by the wrapper class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dccb92a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define own KNN explainer that provides the k nearest neighbors to a query point\n",
    "class KnnExplainer():\n",
    "    def __init__(self, data, outcome_name):\n",
    "        features = data.drop(outcome_name, axis=1)\n",
    "        labels = data[outcome_name]\n",
    "        self._knn_model = KNeighborsClassifier().fit(features, labels)\n",
    "        self._data = data\n",
    "        \n",
    "    def explain(self, sample_df):\n",
    "        nei_indices = self._knn_model.kneighbors(X=sample_df, return_distance=False)\n",
    "        neighbors = self._data.iloc[nei_indices[0], :]\n",
    "        return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773a4d5a",
   "metadata": {},
   "source": [
    "Then, we implement the `train_explainer` and `membership_inference_attack_no_model_access` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a58eda4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KnnMembershipInference(MembershipInference):\n",
    "    def train_explainer(self, data_train, model):\n",
    "        return KnnExplainer(data_train, self.outcome_name)\n",
    "        \n",
    "    @staticmethod\n",
    "    def membership_inference_attack_no_model_access(explainer, samples_df):\n",
    "        inferred_membership = np.empty(len(samples_df))\n",
    "        \n",
    "        for index in range(len(samples_df)):\n",
    "            # needs double brackets so that iloc returns a dataframe instead of a series\n",
    "            sample_df = samples_df.iloc[[index], :]\n",
    "\n",
    "            logger.debug(f'Checking sample {index}: {sample_df.to_numpy()[0]}')\n",
    "\n",
    "            # explainer does not need target for explanation (remove last column)\n",
    "            neighbors = explainer.explain(sample_df.drop(sample_df.columns[-1], axis=1))\n",
    "            \n",
    "            logger.debug(f'K nearest neighbors: \\n {neighbors.to_numpy()}')\n",
    "            \n",
    "            # check if the sample itself is part of it's own nearest neighbors. In that case, it is part of the training data.\n",
    "            # otherwise it isn't.\n",
    "            result = np.isclose(neighbors.to_numpy().astype(float), sample_df.to_numpy().astype(float)).all(axis=1).any()\n",
    "\n",
    "            logger.debug('Inferred membership: %s' % result)\n",
    "            inferred_membership[index] = result\n",
    "        \n",
    "        return inferred_membership"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ef61e5",
   "metadata": {},
   "source": [
    "# Dataset 1: Heart Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a2083",
   "metadata": {},
   "source": [
    "We now generate a KNN explaination for a random sample from the training data as a demonstration of the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64c4cf59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given sample: [[ 68.   1.   3. 150. 195.   1.   0. 132.   0.   0.   4.   4.   6.]]\n",
      "Model prediction: 1.0\n",
      "Nearest Neighbors: \n",
      " [[ 68.    1.    3.  150.  195.    1.    0.  132.    0.    0.    4.    4.\n",
      "    6.    1. ]\n",
      " [ 64.    1.    4.  150.  193.    0.    1.  135.    1.    0.5   2.    4.\n",
      "    8.    1. ]\n",
      " [ 68.    1.    4.  144.  193.    1.    0.  141.    0.    3.4   2.    2.\n",
      "    7.    1. ]\n",
      " [ 54.    1.    2.  160.  195.    0.    1.  130.    0.    1.    1.    4.\n",
      "    8.    0. ]\n",
      " [ 55.    1.    4.  140.  201.    0.    0.  130.    1.    3.    2.    4.\n",
      "    8.    1. ]]\n"
     ]
    }
   ],
   "source": [
    "features = data_heart.drop(outcome_name_heart, axis=1)\n",
    "labels = data_heart[outcome_name_heart]\n",
    "\n",
    "# Train a random forest on training data.\n",
    "model = es.RandomForestClassifier(random_state=0)\n",
    "model = model.fit(features, labels)\n",
    "\n",
    "# Train explainer\n",
    "exp = KnnExplainer(data_heart, outcome_name_heart)\n",
    "\n",
    "given_sample = features.sample()\n",
    "print(f'Given sample: {given_sample.to_numpy()}')\n",
    "\n",
    "pred = model.predict(given_sample)\n",
    "print(f'Model prediction: {pred[0]}')\n",
    "\n",
    "neighbors = exp.explain(given_sample)\n",
    "print(f'Nearest Neighbors: \\n {neighbors.to_numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084cf1e0",
   "metadata": {},
   "source": [
    "We will now do a small proof of concept of the experiment with logging enabled to demonstrate how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cacf1093",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:xai-privacy:[[ 57.   1.   4. 140. 214.   0.   1. 144.   1.   2.   2.   4.   6.   1.]] taken from training data\n",
      "DEBUG:xai-privacy:[[ 63.   1.   4. 160. 230.   1.   0. 105.   1.   1.   2.   4.   8.   1.]] taken from test data\n",
      "DEBUG:xai-privacy:[[ 67.   1.   1. 145.   0.   0.   2. 125.   0.   0.   2.   4.   3.   1.]] taken from training data\n",
      "DEBUG:xai-privacy:[[ 67.   1.   4. 160. 384.   1.   1. 130.   1.   0.   2.   4.   8.   1.]] taken from test data\n",
      "DEBUG:xai-privacy:[[ 66.    1.    4.  120.  302.    0.    2.  151.    0.    0.4   2.    0.\n",
      "    3.    0. ]] taken from training data\n",
      "DEBUG:xai-privacy:[[ 53.   1.   4.  80.   0.   2.   0. 141.   1.   2.   3.   4.   8.   0.]] taken from test data\n",
      "DEBUG:xai-privacy:[[ 50.   1.   4. 144. 349.   0.   2. 120.   1.   1.   1.   4.   7.   1.]] taken from training data\n",
      "DEBUG:xai-privacy:[[ 41.   0.   3. 112. 268.   0.   2. 172.   1.   0.   1.   0.   3.   0.]] taken from test data\n",
      "DEBUG:xai-privacy:[[ 55.   0.   2. 110. 344.   0.   1. 160.   0.   0.   4.   4.   8.   0.]] taken from training data\n",
      "DEBUG:xai-privacy:[[ 54.   1.   4. 150. 365.   0.   1. 134.   0.   1.   1.   4.   8.   0.]] taken from test data\n",
      "DEBUG:xai-privacy:Checking sample 0: [ 57.   1.   4. 140. 214.   0.   1. 144.   1.   2.   2.   4.   6.   1.]\n",
      "DEBUG:xai-privacy:K nearest neighbors: \n",
      " [[ 57.    1.    4.  140.  214.    0.    1.  144.    1.    2.    2.    4.\n",
      "    6.    1. ]\n",
      " [ 54.    1.    4.  136.  220.    0.    0.  140.    1.    3.    2.    4.\n",
      "    8.    1. ]\n",
      " [ 50.    1.    4.  150.  215.    0.    0.  140.    1.    0.    4.    4.\n",
      "    8.    0. ]\n",
      " [ 69.    1.    4.  140.  208.    0.    1.  140.    1.    2.    4.    4.\n",
      "    8.    1. ]\n",
      " [ 58.    1.    2.  125.  220.    0.    0.  144.    0.    0.4   2.    4.\n",
      "    7.    0. ]]\n",
      "DEBUG:xai-privacy:Inferred membership: True\n",
      "DEBUG:xai-privacy:Checking sample 1: [ 63.   1.   4. 160. 230.   1.   0. 105.   1.   1.   2.   4.   8.   1.]\n",
      "DEBUG:xai-privacy:K nearest neighbors: \n",
      " [[ 65.    1.    4.  150.  236.    1.    1.  105.    1.    0.    4.    4.\n",
      "    8.    1. ]\n",
      " [ 65.    0.    4.  150.  225.    0.    2.  114.    0.    1.    2.    3.\n",
      "    7.    1. ]\n",
      " [ 65.    1.    4.  150.  235.    0.    0.  120.    1.    1.5   2.    4.\n",
      "    8.    1. ]\n",
      " [ 47.    1.    4.  150.  226.    0.    0.   98.    1.    1.5   2.    0.\n",
      "    7.    1. ]\n",
      " [ 62.    1.    4.  158.  210.    1.    0.  112.    1.    3.    3.    4.\n",
      "    8.    1. ]]\n",
      "DEBUG:xai-privacy:Inferred membership: False\n",
      "DEBUG:xai-privacy:Checking sample 2: [ 67.   1.   1. 145.   0.   0.   2. 125.   0.   0.   2.   4.   3.   1.]\n",
      "DEBUG:xai-privacy:K nearest neighbors: \n",
      " [[ 67.    1.    1.  145.    0.    0.    2.  125.    0.    0.    2.    4.\n",
      "    3.    1. ]\n",
      " [ 74.    1.    2.  145.    0.    2.    1.  123.    0.    1.3   1.    4.\n",
      "    8.    1. ]\n",
      " [ 68.    1.    4.  138.    0.    0.    0.  130.    1.    3.    2.    4.\n",
      "    8.    1. ]\n",
      " [ 61.    1.    4.  150.    0.    0.    0.  117.    1.    2.    2.    4.\n",
      "    7.    1. ]\n",
      " [ 57.    1.    4.  140.    0.    0.    0.  120.    1.    2.    2.    4.\n",
      "    6.    1. ]]\n",
      "DEBUG:xai-privacy:Inferred membership: True\n",
      "DEBUG:xai-privacy:Checking sample 3: [ 67.   1.   4. 160. 384.   1.   1. 130.   1.   0.   2.   4.   8.   1.]\n",
      "DEBUG:xai-privacy:K nearest neighbors: \n",
      " [[ 56.    1.    4.  170.  388.    0.    1.  122.    1.    2.    2.    4.\n",
      "    8.    1. ]\n",
      " [ 40.    0.    4.  150.  392.    0.    0.  130.    0.    2.    2.    4.\n",
      "    6.    1. ]\n",
      " [ 58.    0.    2.  180.  393.    0.    0.  110.    1.    1.    2.    4.\n",
      "    7.    1. ]\n",
      " [ 65.    0.    3.  160.  360.    0.    2.  151.    0.    0.8   1.    0.\n",
      "    3.    0. ]\n",
      " [ 63.    0.    4.  150.  407.    0.    2.  154.    0.    4.    2.    3.\n",
      "    7.    1. ]]\n",
      "DEBUG:xai-privacy:Inferred membership: False\n",
      "DEBUG:xai-privacy:Checking sample 4: [ 66.    1.    4.  120.  302.    0.    2.  151.    0.    0.4   2.    0.\n",
      "   3.    0. ]\n",
      "DEBUG:xai-privacy:K nearest neighbors: \n",
      " [[ 66.    1.    4.  120.  302.    0.    2.  151.    0.    0.4   2.    0.\n",
      "    3.    0. ]\n",
      " [ 77.    1.    4.  125.  304.    0.    2.  162.    1.    0.    1.    3.\n",
      "    3.    1. ]\n",
      " [ 51.    0.    3.  120.  295.    0.    2.  157.    0.    0.6   1.    0.\n",
      "    3.    0. ]\n",
      " [ 57.    1.    4.  130.  311.    2.    1.  148.    1.    2.    2.    4.\n",
      "    8.    1. ]\n",
      " [ 63.    1.    4.  130.  308.    0.    0.  138.    1.    2.    2.    4.\n",
      "    8.    1. ]]\n",
      "DEBUG:xai-privacy:Inferred membership: True\n",
      "DEBUG:xai-privacy:Checking sample 5: [ 53.   1.   4.  80.   0.   2.   0. 141.   1.   2.   3.   4.   8.   0.]\n",
      "DEBUG:xai-privacy:K nearest neighbors: \n",
      " [[ 64.    0.    4.   95.    0.    2.    0.  145.    0.    1.1   3.    4.\n",
      "    8.    1. ]\n",
      " [ 40.    1.    4.   95.    0.    2.    1.  144.    0.    0.    1.    4.\n",
      "    8.    1. ]\n",
      " [ 51.    1.    4.   95.    0.    2.    0.  126.    0.    2.2   2.    4.\n",
      "    8.    1. ]\n",
      " [ 46.    1.    4.  100.    0.    2.    1.  133.    0.   -2.6   2.    4.\n",
      "    8.    1. ]\n",
      " [ 43.    1.    4.  100.    0.    2.    0.  122.    0.    1.5   3.    4.\n",
      "    8.    1. ]]\n",
      "DEBUG:xai-privacy:Inferred membership: False\n",
      "DEBUG:xai-privacy:Checking sample 6: [ 50.   1.   4. 144. 349.   0.   2. 120.   1.   1.   1.   4.   7.   1.]\n",
      "DEBUG:xai-privacy:K nearest neighbors: \n",
      " [[ 50.    1.    4.  144.  349.    0.    2.  120.    1.    1.    1.    4.\n",
      "    7.    1. ]\n",
      " [ 55.    1.    4.  132.  353.    0.    0.  132.    1.    1.2   2.    1.\n",
      "    7.    1. ]\n",
      " [ 59.    0.    4.  130.  338.    1.    1.  130.    1.    1.5   2.    4.\n",
      "    8.    1. ]\n",
      " [ 41.    1.    4.  120.  336.    0.    0.  118.    1.    3.    2.    4.\n",
      "    8.    1. ]\n",
      " [ 59.    1.    3.  130.  318.    0.    0.  120.    1.    1.    2.    4.\n",
      "    3.    0. ]]\n",
      "DEBUG:xai-privacy:Inferred membership: True\n",
      "DEBUG:xai-privacy:Checking sample 7: [ 41.   0.   3. 112. 268.   0.   2. 172.   1.   0.   1.   0.   3.   0.]\n",
      "DEBUG:xai-privacy:K nearest neighbors: \n",
      " [[ 44.    1.    2.  120.  263.    0.    0.  173.    0.    0.    1.    0.\n",
      "    7.    0. ]\n",
      " [ 49.    1.    3.  115.  265.    0.    0.  175.    0.    0.    4.    4.\n",
      "    8.    1. ]\n",
      " [ 48.    1.    4.  124.  274.    0.    2.  166.    0.    0.5   2.    0.\n",
      "    7.    1. ]\n",
      " [ 36.    1.    2.  120.  267.    0.    0.  160.    0.    3.    2.    4.\n",
      "    8.    1. ]\n",
      " [ 38.    1.    4.  120.  282.    0.    0.  170.    0.    0.    4.    4.\n",
      "    8.    1. ]]\n",
      "DEBUG:xai-privacy:Inferred membership: False\n",
      "DEBUG:xai-privacy:Checking sample 8: [ 55.   0.   2. 110. 344.   0.   1. 160.   0.   0.   4.   4.   8.   0.]\n",
      "DEBUG:xai-privacy:K nearest neighbors: \n",
      " [[ 55.    0.    2.  110.  344.    0.    1.  160.    0.    0.    4.    4.\n",
      "    8.    0. ]\n",
      " [ 57.    0.    4.  120.  354.    0.    0.  163.    1.    0.6   1.    0.\n",
      "    3.    0. ]\n",
      " [ 58.    0.    3.  120.  340.    0.    0.  172.    0.    0.    1.    0.\n",
      "    3.    0. ]\n",
      " [ 54.    0.    4.  127.  333.    1.    1.  154.    0.    0.    4.    4.\n",
      "    8.    1. ]\n",
      " [ 55.    0.    2.  122.  320.    0.    0.  155.    0.    0.    4.    4.\n",
      "    8.    0. ]]\n",
      "DEBUG:xai-privacy:Inferred membership: True\n",
      "DEBUG:xai-privacy:Checking sample 9: [ 54.   1.   4. 150. 365.   0.   1. 134.   0.   1.   1.   4.   8.   0.]\n",
      "DEBUG:xai-privacy:K nearest neighbors: \n",
      " [[ 55.    1.    4.  132.  353.    0.    0.  132.    1.    1.2   2.    1.\n",
      "    7.    1. ]\n",
      " [ 50.    1.    4.  144.  349.    0.    2.  120.    1.    1.    1.    4.\n",
      "    7.    1. ]\n",
      " [ 65.    0.    3.  160.  360.    0.    2.  151.    0.    0.8   1.    0.\n",
      "    3.    0. ]\n",
      " [ 40.    0.    4.  150.  392.    0.    0.  130.    0.    2.    2.    4.\n",
      "    6.    1. ]\n",
      " [ 56.    1.    4.  170.  388.    0.    1.  122.    1.    2.    2.    4.\n",
      "    8.    1. ]]\n",
      "DEBUG:xai-privacy:Inferred membership: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.21s (training model: 0.02s, training explainer: 0.00s, experiment: 0.18s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "EXP = KnnMembershipInference(data_heart, numeric_features_heart, outcome_name_heart, random_state=0)\n",
    "EXP.membership_inference_experiment(num_queries=10, model=DecisionTreeClassifier(random_state=0), model_access=False, threads=1)\n",
    "\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9ba36",
   "metadata": {},
   "source": [
    "The proof of concept should show that the membership inference function predicts membership very accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8971c1",
   "metadata": {},
   "source": [
    "Now we begin executing the actual experiment. We begin by defining the table that will hold the results for all our different experiment variations. Then we execute all variations of the experiment for this dataset. We vary the model between a decision tree, a random forest and a neural network. Each model uses the default configuration of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10674b35",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_ = {'dataset': [], 'model': [], 'accuracy': [], 'precision': [], 'recall': []}\n",
    "\n",
    "results = pd.DataFrame(data = results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3f11f2c-e5e5-4d5f-b4e8-0c2a53bf5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dicts = [data_heart_dict, data_heart_num_dict, data_heart_cat_dict, data_census_dict, data_census_num_dict, data_census_cat_dict]\n",
    "\n",
    "dt_dict = {'name': 'decision tree', 'model': DecisionTreeClassifier}\n",
    "rf_dict = {'name': 'random forest', 'model': es.RandomForestClassifier}\n",
    "nn_dict = {'name': 'neural network', 'model': MLPClassifier}\n",
    "\n",
    "model_dicts = [dt_dict, rf_dict, nn_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c8661dd-d019-4034-9a0b-15365aee7065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: heart, model: decision tree\n",
      "Total time: 0.88s (training model: 0.02s, training explainer: 0.00s, experiment: 0.85s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: heart, model: random forest\n",
      "Total time: 0.88s (training model: 0.16s, training explainer: 0.00s, experiment: 0.72s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: heart, model: neural network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 1.34s (training model: 0.69s, training explainer: 0.00s, experiment: 0.65s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: heart numeric, model: decision tree\n",
      "Total time: 0.47s (training model: 0.01s, training explainer: 0.00s, experiment: 0.46s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: heart numeric, model: random forest\n",
      "Total time: 0.62s (training model: 0.17s, training explainer: 0.00s, experiment: 0.46s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: heart numeric, model: neural network\n",
      "Total time: 1.09s (training model: 0.41s, training explainer: 0.00s, experiment: 0.67s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: heart categorical, model: decision tree\n",
      "Total time: 0.52s (training model: 0.01s, training explainer: 0.00s, experiment: 0.50s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: heart categorical, model: random forest\n",
      "Total time: 0.90s (training model: 0.19s, training explainer: 0.00s, experiment: 0.70s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: heart categorical, model: neural network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 1.61s (training model: 0.99s, training explainer: 0.00s, experiment: 0.62s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: census, model: decision tree\n",
      "Total time: 10.31s (training model: 0.47s, training explainer: 0.02s, experiment: 9.82s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: census, model: random forest\n",
      "Total time: 16.19s (training model: 6.43s, training explainer: 0.02s, experiment: 9.74s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: census, model: neural network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 34.83s (training model: 25.13s, training explainer: 0.03s, experiment: 9.67s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: census numeric, model: decision tree\n",
      "Total time: 3.61s (training model: 0.01s, training explainer: 0.00s, experiment: 3.59s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: census numeric, model: random forest\n",
      "Total time: 4.09s (training model: 0.46s, training explainer: 0.00s, experiment: 3.62s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: census numeric, model: neural network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 10.36s (training model: 6.87s, training explainer: 0.01s, experiment: 3.49s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: census categorical, model: decision tree\n",
      "Total time: 7.04s (training model: 0.21s, training explainer: 0.01s, experiment: 6.82s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: census categorical, model: random forest\n",
      "Total time: 9.93s (training model: 3.29s, training explainer: 0.01s, experiment: 6.63s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n",
      "dataset: census categorical, model: neural network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 24.76s (training model: 17.98s, training explainer: 0.02s, experiment: 6.76s)\n",
      "Accuracy: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# This will run the experiment for each dataset and model combination\n",
    "\n",
    "results = run_all_experiments(KnnMembershipInference, dataset_dicts, model_dicts, random_state=0, num_queries=None, model_access=False, threads=threads, results_table=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1feb9a8",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "The results of all variations of the membership inference experiment with KNN. In every experiment, we executed the membership inference attack on each sample of the training data and each sample of the test data. Both datasets are of equal size and originate from the same source dataset.\n",
    "\n",
    "Accuracy is the percentage of samples whose membership (true or false) was correctly inferred. An algorithm guessing at random would achieve an accuracy of 50 percent.\n",
    "\n",
    "Precision is the percentage of predicted training samples that is actually in the training data.\n",
    "\n",
    "Recall is the percentage of training samples whose membership (true) was correctly inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39bb0af8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heart</td>\n",
       "      <td>decision tree</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heart</td>\n",
       "      <td>random forest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heart</td>\n",
       "      <td>neural network</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heart numeric</td>\n",
       "      <td>decision tree</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heart numeric</td>\n",
       "      <td>random forest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>heart numeric</td>\n",
       "      <td>neural network</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>heart categorical</td>\n",
       "      <td>decision tree</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>heart categorical</td>\n",
       "      <td>random forest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>heart categorical</td>\n",
       "      <td>neural network</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>census</td>\n",
       "      <td>decision tree</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>census</td>\n",
       "      <td>random forest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>census</td>\n",
       "      <td>neural network</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>census numeric</td>\n",
       "      <td>decision tree</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>census numeric</td>\n",
       "      <td>random forest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>census numeric</td>\n",
       "      <td>neural network</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>census categorical</td>\n",
       "      <td>decision tree</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>census categorical</td>\n",
       "      <td>random forest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>census categorical</td>\n",
       "      <td>neural network</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dataset           model  accuracy  precision  recall\n",
       "0                heart   decision tree       1.0        1.0     1.0\n",
       "1                heart   random forest       1.0        1.0     1.0\n",
       "2                heart  neural network       1.0        1.0     1.0\n",
       "3        heart numeric   decision tree       1.0        1.0     1.0\n",
       "4        heart numeric   random forest       1.0        1.0     1.0\n",
       "5        heart numeric  neural network       1.0        1.0     1.0\n",
       "6    heart categorical   decision tree       1.0        1.0     1.0\n",
       "7    heart categorical   random forest       1.0        1.0     1.0\n",
       "8    heart categorical  neural network       1.0        1.0     1.0\n",
       "9               census   decision tree       1.0        1.0     1.0\n",
       "10              census   random forest       1.0        1.0     1.0\n",
       "11              census  neural network       1.0        1.0     1.0\n",
       "12      census numeric   decision tree       1.0        1.0     1.0\n",
       "13      census numeric   random forest       1.0        1.0     1.0\n",
       "14      census numeric  neural network       1.0        1.0     1.0\n",
       "15  census categorical   decision tree       1.0        1.0     1.0\n",
       "16  census categorical   random forest       1.0        1.0     1.0\n",
       "17  census categorical  neural network       1.0        1.0     1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31380dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.to_csv('results/1-4-knn-membership-inference-results.csv', index=False, na_rep='NaN', float_format='%.3f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776fa6ec-8416-4947-aafd-e528c82f9bbb",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "Just as expected, the attack has an accuracy of 100%. No false positives or false negatives occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68b48c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
