{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e6f6fdd-da7e-4d95-ad7d-5a8717aa7823",
   "metadata": {},
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4de9dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import logging\n",
    "import random\n",
    "import multiprocessing\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c7fd929c-d7cb-4c29-beea-0af4d6cf32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "\n",
    "logger = logging.getLogger('xai-privacy')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a85454-00f8-4290-b149-a97835a379f9",
   "metadata": {},
   "source": [
    "This notebook is run before every other notebook in order to take care of general setup functions that are always the same. It loads the two datasets that we use in our experiments and defines the generic classes that execute the experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bbccb8",
   "metadata": {},
   "source": [
    "# Dataset 1: Heart Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538cc8e",
   "metadata": {},
   "source": [
    "Load dataset one: heart disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "32a4a615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:xai-privacy:Loading dataset 1: heart disease (numeric features) ...\n"
     ]
    }
   ],
   "source": [
    "logger.info('Loading dataset 1: heart disease (numeric features) ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cb5934dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_heart = ['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', 'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope', 'CA', 'Thal', 'HeartDisease']\n",
    "\n",
    "filename_cleveland = '../data/processed.cleveland.data'\n",
    "data_cleveland = pd.read_csv(filename_cleveland, names=columns_heart)\n",
    "\n",
    "filename_hungarian = '../data/processed.hungarian.data'\n",
    "data_hungarian = pd.read_csv(filename_hungarian, names=columns_heart)\n",
    "\n",
    "filename_switzerland = '../data/processed.switzerland.data'\n",
    "data_switzerland = pd.read_csv(filename_switzerland, names=columns_heart)\n",
    "\n",
    "filename_va = '../data/processed.va.data'\n",
    "data_va = pd.read_csv(filename_va, names=columns_heart)\n",
    "\n",
    "filename_stalog = '../data/heart.dat'\n",
    "data_stalog = pd.read_csv(filename_stalog, sep=' ', names=columns_heart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920dcaa3",
   "metadata": {},
   "source": [
    "For this dataset we only look at numerical data so we drop the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e63a01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features_heart = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
    "all_features_heart = ['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', 'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope', 'CA', 'Thal']\n",
    "outcome_name_heart = 'HeartDisease'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12edd6a-77ff-4fa7-8ec5-ea83ab470ef3",
   "metadata": {},
   "source": [
    "Fix the target column so that 0 is no heart disease, 1 is heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ff63731f-9a3a-4ebb-b1a3-eeb1a5daf4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_target(df):\n",
    "    df.loc[(df[outcome_name_heart] > 0), outcome_name_heart] = 1\n",
    "    return df\n",
    "    \n",
    "data_cleveland = fix_target(data_cleveland)\n",
    "data_switzerland = fix_target(data_switzerland)\n",
    "data_va = fix_target(data_va)\n",
    "\n",
    "data_stalog.loc[(data_stalog[outcome_name_heart] == 1), outcome_name_heart] = 0\n",
    "data_stalog.loc[(data_stalog[outcome_name_heart] == 2), outcome_name_heart] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf84fb8a-2f06-401a-be8e-85abd640bd9b",
   "metadata": {},
   "source": [
    "Combine all 5 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b319bbdd-188c-443f-9ce9-9d2265765426",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_heart = pd.concat([data_cleveland, data_hungarian, data_switzerland, data_va, data_stalog])\n",
    "data_heart = data_heart.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a47a2f-513f-4d0e-94b4-1a85f20ac8c6",
   "metadata": {},
   "source": [
    "Remove rows with missing numeric values. Add new categories for missing categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2dd83903-6744-416e-a3f3-fcdd30855b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Age: removed 0 rows for missing values.\n",
      "Feature RestingBP: removed 59 rows for missing values.\n",
      "Feature Cholesterol: removed 27 rows for missing values.\n",
      "Feature FastingBS: add unknown category 2.0\n",
      "Feature RestingECG: add unknown category 3.0\n",
      "Feature MaxHR: removed 0 rows for missing values.\n",
      "Feature Oldpeak: removed 7 rows for missing values.\n",
      "Feature ST_Slope: add unknown category 4.0\n",
      "Feature CA: add unknown category 4.0\n",
      "Feature Thal: add unknown category 8.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>CA</th>\n",
       "      <th>Thal</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n",
       "0  47.0  1.0            1.0      110.0        249.0        0.0         0.0   \n",
       "1  47.0  1.0            4.0      110.0        275.0        0.0         2.0   \n",
       "2  44.0  0.0            3.0      118.0        242.0        0.0         0.0   \n",
       "3  70.0  1.0            4.0      130.0        322.0        0.0         2.0   \n",
       "4  63.0  1.0            1.0      145.0        233.0        1.0         2.0   \n",
       "\n",
       "   MaxHR  ExerciseAngina  Oldpeak  ST_Slope   CA  Thal  HeartDisease  \n",
       "0  150.0             0.0      0.0       4.0  4.0   8.0           0.0  \n",
       "1  118.0             1.0      1.0       2.0  1.0   3.0           1.0  \n",
       "2  149.0             0.0      0.3       2.0  1.0   3.0           0.0  \n",
       "3  109.0             0.0      2.4       2.0  3.0   3.0           1.0  \n",
       "4  150.0             0.0      2.3       3.0  0.0   6.0           0.0  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for feature in all_features_heart:\n",
    "    if feature in numeric_features_heart:\n",
    "        # remove rows with missing numeric value\n",
    "        non_empty_indices = data_heart[feature] != '?'\n",
    "        len_before_removal = len(data_heart)\n",
    "        data_heart = data_heart[non_empty_indices]\n",
    "        print(f'Feature {feature}: removed {len_before_removal - len(data_heart)} rows for missing values.')\n",
    "    else:\n",
    "        # add category \"unknown\" if categorical feature with missing values\n",
    "        empty_indices = data_heart[feature] == '?'\n",
    "        if empty_indices.any():\n",
    "            unique_values = data_heart[feature].unique().tolist()\n",
    "            unique_values.remove('?')\n",
    "            unique_values = [float(i) for i in unique_values]\n",
    "            max_category = max(unique_values)\n",
    "            unknown_category = max_category + 1\n",
    "            data_heart[feature] = data_heart[feature].replace('?', unknown_category)\n",
    "            print(f'Feature {feature}: add unknown category {unknown_category}')\n",
    "\n",
    "data_heart = data_heart.astype(float)\n",
    "            \n",
    "data_heart.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dc51362b-de3b-4d04-b09b-858b9ce85591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  RestingBP  Cholesterol  MaxHR  Oldpeak  HeartDisease\n",
       "0  47.0      110.0        249.0  150.0      0.0           0.0\n",
       "1  47.0      110.0        275.0  118.0      1.0           1.0\n",
       "2  44.0      118.0        242.0  149.0      0.3           0.0\n",
       "3  70.0      130.0        322.0  109.0      2.4           1.0\n",
       "4  63.0      145.0        233.0  150.0      2.3           0.0"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_heart_num = data_heart.drop('Sex', axis=1).drop('ChestPainType', axis=1).drop('FastingBS', axis=1).drop('RestingECG', axis=1).drop('ExerciseAngina', axis=1).drop('ST_Slope', axis=1).drop('CA', axis=1).drop('Thal', axis=1)\n",
    "\n",
    "numeric_features_heart_num = numeric_features_heart\n",
    "all_features_heart_num = numeric_features_heart_num\n",
    "\n",
    "data_heart_num.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4b10a1a6-71f5-4d4b-a785-2cd0232b067a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>CA</th>\n",
       "      <th>Thal</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n",
       "0  3.0  1.0            1.0        3.0          4.0        0.0         0.0   \n",
       "1  3.0  1.0            4.0        3.0          4.0        0.0         2.0   \n",
       "2  3.0  0.0            3.0        3.0          4.0        0.0         0.0   \n",
       "3  8.0  1.0            4.0        4.0          5.0        0.0         2.0   \n",
       "4  7.0  1.0            1.0        5.0          3.0        1.0         2.0   \n",
       "\n",
       "   MaxHR  ExerciseAngina  Oldpeak  ST_Slope   CA  Thal  HeartDisease  \n",
       "0    6.0             0.0      2.0       4.0  4.0   8.0           0.0  \n",
       "1    4.0             1.0      4.0       2.0  1.0   3.0           1.0  \n",
       "2    6.0             0.0      3.0       2.0  1.0   3.0           0.0  \n",
       "3    3.0             0.0      5.0       2.0  3.0   3.0           1.0  \n",
       "4    6.0             0.0      5.0       3.0  0.0   6.0           0.0  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_heart_cat = data_heart.copy()\n",
    "for feature in numeric_features_heart:\n",
    "    # we discretize the numeric features into 10 bins of equal width\n",
    "    data_heart_cat[feature] = pd.cut(data_heart_cat[feature], 10)\n",
    "    # represent categories as numbers (expected by experiment code later on)\n",
    "    data_heart_cat[feature] = OrdinalEncoder(dtype=float).fit_transform(data_heart_cat[[feature]])\n",
    "    \n",
    "numeric_features_heart_cat = []\n",
    "all_features_heart_cat = all_features_heart\n",
    "\n",
    "data_heart_cat.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075a3758-7319-40fb-bfcc-311f6c182bc0",
   "metadata": {},
   "source": [
    "We also drop any duplicate rows. We only consider the feature columns for duplicates. That means if there are two rows with identical features but different outcomes (labels), then they will still be dropped. This is important so that the metrics for membership inference and training data extraction can later be accurately measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7eca1efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 271 of 1097\n",
      "Dropped 273 of 1097\n",
      "Dropped 277 of 1097\n"
     ]
    }
   ],
   "source": [
    "len_heart_before = len(data_heart)\n",
    "len_heart_num_before = len(data_heart_num)\n",
    "len_heart_cat_before = len(data_heart_cat)\n",
    "\n",
    "data_heart = data_heart.drop_duplicates(subset=all_features_heart)\n",
    "data_heart_num = data_heart_num.drop_duplicates(subset=all_features_heart_num)\n",
    "data_heart_cat = data_heart_cat.drop_duplicates(subset=all_features_heart_cat)\n",
    "\n",
    "print(f'Dropped {len_heart_before - len(data_heart)} of {len_heart_before}')\n",
    "print(f'Dropped {len_heart_num_before - len(data_heart_num)} of {len_heart_num_before}')\n",
    "print(f'Dropped {len_heart_cat_before - len(data_heart_cat)} of {len_heart_cat_before}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70355eeb-e37e-45b8-9562-0ed463e29c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_heart_dict = {'name': 'heart', 'dataset': data_heart, 'num': numeric_features_heart, 'outcome': outcome_name_heart}\n",
    "data_heart_num_dict = {'name': 'heart numeric', 'dataset': data_heart_num, 'num': numeric_features_heart_num, 'outcome': outcome_name_heart}\n",
    "data_heart_cat_dict = {'name': 'heart categorical', 'dataset': data_heart_cat, 'num': numeric_features_heart_cat, 'outcome': outcome_name_heart}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5adf67",
   "metadata": {},
   "source": [
    "# Dataset 2: Census Income"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07965e56",
   "metadata": {},
   "source": [
    "Load dataset two: census income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f645e0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:xai-privacy:Loading dataset 2: census income (categorical features) ...\n"
     ]
    }
   ],
   "source": [
    "logger.info('Loading dataset 2: census income (categorical features) ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "bb6dedf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_census = '../data/adult.data.csv'\n",
    "\n",
    "all_features_census = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', \\\n",
    "         'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'label']\n",
    "\n",
    "data_census = pd.read_csv(filename_census, names=all_features_census)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995fa5d9",
   "metadata": {},
   "source": [
    "There is missing data in the columns workclass, native_country and occupation that needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8f0b8259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped: 2399 of 32561\n"
     ]
    }
   ],
   "source": [
    "len_before = len(data_census)\n",
    "\n",
    "data_census = data_census[data_census.workclass != ' ?']\n",
    "data_census = data_census[data_census.native_country != ' ?']\n",
    "data_census = data_census[data_census.occupation != ' ?']\n",
    "\n",
    "print(f'Dropped: {len_before - len(data_census)} of {len_before}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba969a1",
   "metadata": {},
   "source": [
    "Transform workclass, education, marital_status, occupation, relationship, race, sex and native_country into ordinal encoded features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "bfd74e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  education  education_num  marital_status  occupation  \\\n",
       "0   39          5          9             13               4           0   \n",
       "1   50          4          9             13               2           3   \n",
       "2   38          2         11              9               0           5   \n",
       "\n",
       "   relationship  race  sex  capital_gain  capital_loss  hours_per_week  \\\n",
       "0             1     4    1          2174             0              40   \n",
       "1             0     4    1             0             0              13   \n",
       "2             1     4    1             0             0              40   \n",
       "\n",
       "   native_country  income  \n",
       "0              38       0  \n",
       "1              38       0  \n",
       "2              38       0  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_census[['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']] =\\\n",
    "    OrdinalEncoder(dtype=int).fit_transform(data_census[['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex',\\\n",
    "                                                      'native_country']])\n",
    "\n",
    "data_census['income'] = LabelEncoder().fit_transform(data_census['label'])\n",
    "\n",
    "data_census = data_census.drop('label', axis=1)\n",
    "data_census = data_census.drop('fnlwgt', axis=1)\n",
    "\n",
    "numeric_features_census = ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "\n",
    "all_features_census = ['age', 'workclass', 'education', 'education_num', 'marital_status', 'occupation', \\\n",
    "         'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country']\n",
    "\n",
    "outcome_name_census = 'income'\n",
    "    \n",
    "data_census.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d284cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will only use the categorical features of this dataset. Remove numeric columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "69628566",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>education_num</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  education_num  capital_gain  capital_loss  hours_per_week  income\n",
       "0   39             13          2174             0              40       0\n",
       "1   50             13             0             0              13       0\n",
       "2   38              9             0             0              40       0"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_census_num = data_census.drop('workclass', axis=1).drop('education', axis=1).drop('marital_status', axis=1).drop('occupation', axis=1).drop('relationship', axis=1).drop('race', axis=1).drop('sex', axis=1).drop('native_country', axis=1)\n",
    "\n",
    "numeric_features_census_num = numeric_features_census\n",
    "all_features_census_num = numeric_features_census_num\n",
    "\n",
    "data_census_num.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "98b70a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  education  education_num  marital_status  occupation  \\\n",
       "0    3          5          9              7               4           0   \n",
       "1    4          4          9              7               2           3   \n",
       "2    2          2         11              5               0           5   \n",
       "\n",
       "   relationship  race  sex  capital_gain  capital_loss  hours_per_week  \\\n",
       "0             1     4    1             0             0               3   \n",
       "1             0     4    1             0             0               1   \n",
       "2             1     4    1             0             0               3   \n",
       "\n",
       "   native_country  income  \n",
       "0              38       0  \n",
       "1              38       0  \n",
       "2              38       0  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_census_cat = data_census.copy()\n",
    "for feature in numeric_features_census:\n",
    "    # we discretize the numeric features into 10 bins of equal width\n",
    "    data_census_cat[feature] = pd.cut(data_census_cat[feature], 10)\n",
    "    # represent categories as numbers (expected by experiment code later on)\n",
    "    data_census_cat[feature] = OrdinalEncoder(dtype=int).fit_transform(data_census_cat[[feature]])\n",
    "\n",
    "numeric_features_census_cat = []\n",
    "all_features_census_cat = all_features_census\n",
    "\n",
    "data_census_cat.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1cb2f5",
   "metadata": {},
   "source": [
    "Drop duplicates. We only consider the feature columns for duplicates. That means if there are two rows with identical features but different outcomes (labels), then they will still be dropped. This is important so that the metrics for membership inference and training data extraction can later be accurately measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b735c921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "census: Dropped 0 of 26314\n",
      "num: Dropped 0 of 10303\n",
      "cat: Dropped 8288 of 26314\n"
     ]
    }
   ],
   "source": [
    "len_census_before = len(data_census)\n",
    "len_census_num_before = len(data_census_num)\n",
    "len_census_cat_before = len(data_census_cat)\n",
    "\n",
    "data_census = data_census.drop_duplicates(subset=all_features_census)\n",
    "data_census_num = data_census_num.drop_duplicates(subset=all_features_census_num)\n",
    "data_census_cat = data_census_cat.drop_duplicates(subset=all_features_census_cat)\n",
    "\n",
    "print(f'census: Dropped {len_census_before - len(data_census)} of {len_census_before}')\n",
    "print(f'num: Dropped {len_census_num_before - len(data_census_num)} of {len_census_num_before}')\n",
    "print(f'cat: Dropped {len_census_cat_before - len(data_census_cat)} of {len_census_cat_before}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f36f60b-0aac-4df6-a884-49c08f9729ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_census_dict = {'name': 'census', 'dataset': data_census, 'num': numeric_features_census, 'outcome': outcome_name_census}\n",
    "data_census_num_dict = {'name': 'census numeric', 'dataset': data_census_num, 'num': numeric_features_census_num, 'outcome': outcome_name_census}\n",
    "data_census_cat_dict = {'name': 'census categorical', 'dataset': data_census_cat, 'num': numeric_features_census_cat, 'outcome': outcome_name_census}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181b1b2",
   "metadata": {},
   "source": [
    "# Membership Inference and Training Data Extraction Experiment Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e643bc52-2a12-4cac-bf40-346671c62db7",
   "metadata": {},
   "source": [
    "This is the generic XaiPrivacyExperiment class, which specific experiments will inherit from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2080c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XaiPrivacyExperiment():\n",
    "    \"\"\"Generic framework for an XAI and data privacy experiment\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    rs, rng\n",
    "        Random states for numpy\n",
    "    data\n",
    "        Pandas dataframe of the dataset that the experiment is executed on. Contains features and labels.\n",
    "    numeric_features : list[str]\n",
    "        The numeric feature names of the dataset.\n",
    "    categorical_features : list[str]\n",
    "        The categorical feature names of the dataset.\n",
    "    outcome_name : str\n",
    "        The name of the column that contains the labels.\n",
    "    features\n",
    "        Pandas dataframe that only contains the feature values of all samples (not labels).\n",
    "    labels\n",
    "        Pandas dataframe that only contains the labels of all samples (not features).\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    train_explainer(data_train, model):\n",
    "        Trains the explainer on the given data and model (abstract method).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, numeric_features, outcome_name, random_state: int):\n",
    "        \"\"\"        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data\n",
    "            Pandas dataframe of the dataset that the experiment is executed on. Contains features and labels.\n",
    "        numeric_features : list[str]\n",
    "            The numeric feature names of the dataset.\n",
    "        outcome_name : str\n",
    "            The name of the column that contains the labels.\n",
    "        random_state: int\n",
    "            The seed for all random actions during the experiment (such as drawing samples for membership inference)\n",
    "        \"\"\"\n",
    "        # create random state from seed. This will be used for all random actions (such as drawing samples for membership inference)\n",
    "        self.rs = np.random.RandomState(seed=random_state)\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "        random.seed(random_state)\n",
    "        \n",
    "        self.data = data\n",
    "        self.numeric_features = numeric_features\n",
    "        self.outcome_name = outcome_name\n",
    "    \n",
    "        # split dataset into features and labels.\n",
    "        self.features = self.data.drop(outcome_name, axis=1)\n",
    "        self.labels = self.data[outcome_name]\n",
    "        \n",
    "        # names of the categorical features\n",
    "        self.categorical_features = self.features.columns.difference(numeric_features).tolist()\n",
    "        \n",
    "        logger.debug(f'Numeric Features: {self.numeric_features}')\n",
    "        logger.debug(f'Categorical Features: {self.categorical_features}')\n",
    "    \n",
    "    def _model_pipeline(self, model):\n",
    "        if len(self.categorical_features) > 0 and len(self.numeric_features) > 0:\n",
    "            # Define transformer to one-hot-encode categorical features and numeric features are scaled\n",
    "            numeric_transformer = StandardScaler()\n",
    "            categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"num\", numeric_transformer, self.numeric_features),\n",
    "                    (\"cat\", categorical_transformer, self.categorical_features),\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "        elif len(self.categorical_features) > 0:\n",
    "            # Define transformer to one-hot-encode categorical features\n",
    "            categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"cat\", categorical_transformer, self.categorical_features)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            # Define transformer to scale numeric features\n",
    "            numeric_transformer = StandardScaler()\n",
    "\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"num\", numeric_transformer, self.numeric_features)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "        \n",
    "    def _print_times(self, start_time, model_time, explainer_time, end_time):\n",
    "        print(f'Total time: {end_time - start_time:.2f}s (training model: {model_time - start_time:.2f}s, training explainer: {explainer_time - model_time:.2f}s, experiment: {end_time - explainer_time:.2f}s)')\n",
    "    \n",
    "    def train_explainer(self, data_train, model):\n",
    "        \"\"\"Trains the explainer on the given data and model\n",
    "        \n",
    "        Abstract method that must be implemented by subclass. Returns the explainer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data_train\n",
    "            The training data (features and labels).\n",
    "        model\n",
    "            The trained model that will be explained by the explainer.\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        NotImplementedError\n",
    "            Must be implemented by subclass.\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee6122-650f-404a-81c4-e5dc3731139a",
   "metadata": {},
   "source": [
    "The following two classes are two specific xai privacy experiments: Membership inference and training data extraction. The general structure of these experiments is definied by these classes, however specific attacks must still be implemented by subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7ee03-e5e2-40ac-9280-b67eff3fd5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MembershipInference(XaiPrivacyExperiment):\n",
    "    \"\"\"\n",
    "    Executes a membership inference attack. Some public methods must be implemented by subclass.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    membership_inference_experiment(num_queries: int, model, model_access: bool)\n",
    "        Executes membership inference experiment\n",
    "    membership_inference_attack_model_access(explainer, samples_df, model):\n",
    "        Executes membership inference attack with access to the model\n",
    "    membership_inference_attack_no_model_access(explainer, samples_df):\n",
    "        Executes membership inference attack without access to the model\n",
    "    \"\"\"\n",
    "    \n",
    "    def membership_inference_experiment(self, num_queries: int, model, model_access: bool, threads, pretrained_model_and_explainer=None):\n",
    "        \"\"\"Executes membership inference experiment\n",
    "        \n",
    "        Executes the membership inference experiment with the dataset that this object was instantiated with. Trains given\n",
    "        model on half the dataset and tests accuracy, precision and recall of the implemented membership inference attack.\n",
    "        If model_access is True, the attack method with the parameter \"model\" is used (the attacker has access to the model).\n",
    "        Otherwise, the attack method without that parameter is used (the attacker has no access to the model).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_queries : int\n",
    "            Number of samples that the membership inference attack is attempted on. Should not be greater than len(data).\n",
    "            If None, then membership inference will be attemped on all samples.\n",
    "        model\n",
    "            The untrained model used in the experiment\n",
    "        model_access : bool\n",
    "            Whether the membership inference attack is executed with attacker access to the model or without.\n",
    "        \"\"\"\n",
    "        # stop the time of training model, training explainer, and executing experiment\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if pretrained_model_and_explainer is None:\n",
    "            model, explainer, train_model_time, train_explainer_time, data_train, data_test = self._train_model_and_explainer(model)\n",
    "        else:\n",
    "            model, explainer, train_model_time, train_explainer_time = pretrained_model_and_explainer\n",
    "            data_train, data_test = self._split_data()\n",
    "            \n",
    "        \n",
    "        # draw samples from training and test data. record each sample's membership in training data.\n",
    "        samples_df, actual_membership = self._draw_mi_samples(num_queries, data_train, data_test)\n",
    "            \n",
    "        # infer membership using membership inference attack against the explainer\n",
    "        if threads > 1:\n",
    "            arg_list = self._args_for_parallel_execution(threads, samples_df, explainer, model, model_access)\n",
    "\n",
    "            if model_access:\n",
    "                with multiprocessing.Pool(threads) as p:\n",
    "                    results = p.starmap(self.membership_inference_attack_model_access, arg_list)\n",
    "            else:\n",
    "                with multiprocessing.Pool(threads) as p:\n",
    "                    results = p.starmap(self.membership_inference_attack_no_model_access, arg_list)\n",
    "\n",
    "            inferred_membership = np.concatenate(results, axis=0)\n",
    "        else:\n",
    "            if model_access:\n",
    "                inferred_membership = self.membership_inference_attack_model_access(explainer, samples_df, model)\n",
    "            else:\n",
    "                inferred_membership = self.membership_inference_attack_no_model_access(explainer, samples_df)\n",
    "            \n",
    "            \n",
    "        end_time = time.time()\n",
    "        self._print_times(start_time, train_model_time, train_explainer_time, end_time)\n",
    "\n",
    "        # calculate accuracy, precision and recall\n",
    "        return self._calc_accuracy_precision_recall(actual_membership, inferred_membership)\n",
    "    \n",
    "    def _train_model_and_explainer(self, model):\n",
    "        # create pipeline that transforms categorical features to one hot encoding\n",
    "        model = self._model_pipeline(model)\n",
    "\n",
    "        # split data into two halves (one is used for training and inference, the other only for inference)\n",
    "        data_train, data_test = self._split_data()\n",
    "\n",
    "        # train classifier on training data\n",
    "        model = model.fit(data_train.drop(self.outcome_name, axis=1), data_train[self.outcome_name])\n",
    "        train_model_time = time.time()\n",
    "\n",
    "        # train explainer on training data and classifier\n",
    "        explainer = self.train_explainer(data_train, model)\n",
    "        train_explainer_time = time.time()\n",
    "        \n",
    "        return model, explainer, train_model_time, train_explainer_time, data_train, data_test\n",
    "    \n",
    "    def _split_data(self):\n",
    "        # split data into two halves. One is used for training, the other as test data that is not part of the training data.\n",
    "        # this test data will be needed as membership inference samples that do not belong to the training data.\n",
    "        idx_mid = int(self.features.shape[0] / 2)\n",
    "\n",
    "        data_train = self.data.iloc[idx_mid:, :]\n",
    "        data_test = self.data.iloc[:idx_mid, :]\n",
    "        \n",
    "        # remove test samples that have a category that is not covered by the training samples\n",
    "        len_test_prev = len(data_test)\n",
    "        for feature in self.features.columns:\n",
    "            if feature in self.categorical_features:\n",
    "                unique_train = data_train[feature].unique().tolist()\n",
    "                unique_test = data_test[feature].unique().tolist()\n",
    "\n",
    "                values_not_in_train = [x for x in unique_test if x not in unique_train]\n",
    "                for value in values_not_in_train:\n",
    "                    data_test = data_test[data_test[feature] != value]\n",
    "            else:\n",
    "                min_train = data_train[feature].min()\n",
    "                max_train = data_train[feature].max()\n",
    "                \n",
    "                data_test = data_test[(data_test[feature] >= min_train) & (data_test[feature] <= max_train)]\n",
    "                \n",
    "        logger.debug(f'Removed {len_test_prev - len(data_test)} test samples due to unknown category.')\n",
    "        \n",
    "        return data_train, data_test\n",
    "    \n",
    "    def _draw_mi_samples(self, num_queries, data_train, data_test):\n",
    "        # create new dataframe that will hold all samples for the experiment\n",
    "        samples_df = pd.DataFrame(columns=list(data_train.columns.values), dtype=float)\n",
    "        \n",
    "        if num_queries is None:\n",
    "            num_samples = len(data_train) + len(data_test)\n",
    "        else:\n",
    "            num_samples = num_queries\n",
    "        \n",
    "        # record each sample's actual membership. If the sample comes from the training data -> True. If the sample comes\n",
    "        # from the test data -> False.\n",
    "        sample_membership = np.empty(num_samples)\n",
    "        \n",
    "        if num_queries is None:\n",
    "            # if the experiment is executed on all data, simply concatenate the training and test data. We do not need to randomly draw samples\n",
    "            samples_df = pd.concat([data_train, data_test], ignore_index=True)\n",
    "            sample_membership[:len(data_train)] = True\n",
    "            sample_membership[len(data_train):] = False\n",
    "        else:\n",
    "            # Otherwise, random samples need to be drawn:\n",
    "            # half the samples come from the training data, the other half from the test data\n",
    "            for i in range(num_samples):\n",
    "                if i % 2 == 0:\n",
    "                    # choose sample from training data.\n",
    "                    sample = data_train.sample(random_state=self.rs)\n",
    "                    sample_membership[i] = True\n",
    "                    logger.debug('%s taken from training data' % sample.to_numpy())\n",
    "                else:\n",
    "                    # choose sample from test data.\n",
    "                    sample = data_test.sample(random_state=self.rs)\n",
    "                    sample_membership[i] = False\n",
    "                    logger.debug('%s taken from test data' % sample.to_numpy())\n",
    "\n",
    "                samples_df = pd.concat([samples_df, sample], ignore_index=True)\n",
    "            \n",
    "        return samples_df, sample_membership\n",
    "            \n",
    "    def _args_for_parallel_execution(self, threads, samples_df, explainer, model, model_access):\n",
    "        num_samples = len(samples_df)\n",
    "        \n",
    "        # ceil division. This is equivalent to num_samples / threads (rounded up).\n",
    "        samples_per_thread = -(num_samples // -threads)\n",
    "            \n",
    "        arg_list = []\n",
    "            \n",
    "        for i in range(threads):\n",
    "            start_idx = i * samples_per_thread\n",
    "            end_idx = min((i + 1) * samples_per_thread, num_samples)\n",
    "            \n",
    "            if model_access:\n",
    "                arg_list.append((explainer, samples_df.iloc[start_idx:end_idx, :], model))\n",
    "            else:\n",
    "                arg_list.append((explainer, samples_df.iloc[start_idx:end_idx, :]))\n",
    "            \n",
    "        return arg_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calc_accuracy_precision_recall(actual_membership, inferred_membership):\n",
    "        samples_in_training_data = np.count_nonzero(actual_membership)\n",
    "        samples_not_in_training_data = len(actual_membership) - samples_in_training_data\n",
    "\n",
    "        pred_positives = np.count_nonzero(inferred_membership)\n",
    "\n",
    "        correct_predictions = np.count_nonzero(np.equal(inferred_membership, actual_membership))\n",
    "        true_positives = np.count_nonzero(inferred_membership[actual_membership == True])\n",
    "\n",
    "        accuracy = correct_predictions / len(actual_membership)\n",
    "        if pred_positives > 0:\n",
    "            precision = true_positives / pred_positives\n",
    "        else:\n",
    "            # If the attack predicted membership for no given sample then precision cannot be calculated\n",
    "            precision = float(\"NaN\")\n",
    "        recall = true_positives / samples_in_training_data\n",
    "        \n",
    "        print(f'Accuracy: {accuracy}, precision: {precision}, recall: {recall}')\n",
    "        \n",
    "        return accuracy, precision, recall\n",
    "    \n",
    "    @staticmethod\n",
    "    def membership_inference_attack_model_access(explainer, samples_df, model):\n",
    "        \"\"\"Executes membership inference attack with access to the model\n",
    "        \n",
    "        Abstract method that must be implemented by subclass. Executes the attack against the explainer with access to the \n",
    "        model. Infers membership for each sample in samples_df. Returns a numpy array with boolean values indicating the \n",
    "        inferred membership of each given sample. Must be same length as samples_df.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        explainer\n",
    "            The explainer or explanation that will be attacked.\n",
    "        samples_df\n",
    "            A pandas dataframe that contains the feature values of all given samples.\n",
    "        model\n",
    "            The trained model that is explained by the explainer.\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        NotImplementedError\n",
    "            Must be implemented by subclass.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @staticmethod\n",
    "    def membership_inference_attack_no_model_access(explainer, samples_df, ignore):\n",
    "        \"\"\"Executes membership inference attack without access to the model\n",
    "        \n",
    "        Abstract method that must be implemented by subclass. Executes the attack against the explainer without access to the \n",
    "        model. Infers membership for each sample in samples_df. Returns a numpy array with boolean values indicating the \n",
    "        inferred membership of each given sample. Must be same length as samples_df.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        explainer\n",
    "            The explainer or explanation that will be attacked.\n",
    "        samples_df\n",
    "            A pandas dataframe that contains the feature values of all given samples.\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        NotImplementedError\n",
    "            Must be implemented by subclass.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc760a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataExtraction(XaiPrivacyExperiment):\n",
    "    \"\"\"\n",
    "    Executes a training data extraction attack. Some public methods must be implemented by subclass.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    \n",
    "    training_data_extraction_experiment(self, num_queries: None or int, model, model_access: bool):\n",
    "        Executes training data extraction experiment\n",
    "    training_data_extraction_model_access(explainer, num_queries, feature_format, rng, model):\n",
    "        Executes training data extraction attack with access to the model\n",
    "    training_data_extraction_no_model_access(explainer, num_queries, feature_format, rng):\n",
    "        Executes training data extraction attack without access to the model\n",
    "    \"\"\"\n",
    "    \n",
    "    def training_data_extraction_experiment(self, num_queries: None or int, model, model_access: bool, threads=1):\n",
    "        \"\"\"Executes training data extraction experiment\n",
    "        \n",
    "        Executes the training data extraction experiment with the dataset that this object was instantiated with. Trains given\n",
    "        model on dataset and tests precision and recall of the implemented training data extraction attack.\n",
    "        If model_access is True, the attack method with the parameter \"model\" is used (the attacker has access to the model).\n",
    "        Otherwise, the attack method without that parameter is used (the attacker has no access to the model).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_queries : None or int\n",
    "            The number of queries allowed for the attacker to extract a sample. If None, the attack can make any number\n",
    "            of queries to attempt to extract the full dataset.\n",
    "        model\n",
    "            The untrained model used in the experiment.\n",
    "        model_access : bool\n",
    "            Whether the attack is executed with attacker access to the model or without.\n",
    "        \"\"\"\n",
    "        # stop the time of training model, training explainer, and executing experiment\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # create pipeline that transforms categorical features to one hot encoding\n",
    "        model = self._model_pipeline(model)\n",
    "\n",
    "        # train classifier on dataset\n",
    "        model = model.fit(self.features, self.labels)\n",
    "        \n",
    "        train_model_time = time.time()\n",
    "\n",
    "        # train explainer on training data and classifier\n",
    "        explainer = self.train_explainer(self.data, model)\n",
    "        \n",
    "        train_explainer_time = time.time()\n",
    "        \n",
    "        # generate the feature format information that is available to the attacker\n",
    "        feature_format = self._generate_feature_info(self.features, self.numeric_features)\n",
    "            \n",
    "        # extract samples using training data extraction attack against the explainer\n",
    "        if threads > 1:\n",
    "            arg_list = self._args_for_parallel_execution(threads, explainer, num_queries, feature_format, model, model_access)\n",
    "            \n",
    "            if model_access:\n",
    "                with multiprocessing.Pool(threads) as p:\n",
    "                    results = p.starmap(self.training_data_extraction_model_access, arg_list)\n",
    "            else:\n",
    "                with multiprocessing.Pool(threads) as p:\n",
    "                    results = p.starmap(self.training_data_extraction_no_model_access, arg_list)\n",
    "                    \n",
    "            extracted_samples = pd.concat(results)\n",
    "        else:\n",
    "            if model_access:\n",
    "                extracted_samples = self.training_data_extraction_model_access(explainer, num_queries, feature_format, self.rng, model)\n",
    "            else:\n",
    "                extracted_samples = self.training_data_extraction_no_model_access(explainer, num_queries, feature_format, self.rng)\n",
    "            \n",
    "        # compare the extracted samples to the training data -> number of accurate extractions\n",
    "        accurate_samples, num_extracted_samples, all_samples = self._compare_data(extracted_samples, self.data, num_queries)\n",
    "            \n",
    "        end_time = time.time()\n",
    "        self._print_times(start_time, train_model_time, train_explainer_time, end_time)\n",
    "            \n",
    "        # calculate precision and recall\n",
    "        return self._calc_precision_recall_tde(accurate_samples, num_extracted_samples, all_samples)\n",
    "    \n",
    "    def _args_for_parallel_execution(self, threads, explainer, num_queries, feature_format, model, model_access):\n",
    "        \n",
    "        avg_queries_per_thread = num_queries / threads\n",
    "        \n",
    "        arg_list = []\n",
    "        total_queries = 0\n",
    "        \n",
    "        for i in range(threads):\n",
    "            num_queries_local = int((i+1) * avg_queries_per_thread) - int(i * avg_queries_per_thread)\n",
    "            total_queries += num_queries_local\n",
    "            \n",
    "            if i == threads - 1 and total_queries < num_queries:\n",
    "                num_queries_local += num_queries - total_queries\n",
    "            \n",
    "            if model_access:\n",
    "                arg_list.append((explainer, num_queries_local, feature_format, self.rng.integers(100000), model))\n",
    "            else:\n",
    "                arg_list.append((explainer, num_queries_local, feature_format, self.rng.integers(100000)))\n",
    "                \n",
    "        return arg_list\n",
    "                \n",
    "    \n",
    "    @staticmethod\n",
    "    def _generate_feature_info(features, numeric_features):\n",
    "        feature_information = []\n",
    "        \n",
    "        features_np = features.to_numpy()\n",
    "        \n",
    "        # Get the minimum and maximum value for all numeric features in the training data.\n",
    "        # Get the categories for all categorical features.\n",
    "        for i, feature_name in enumerate(features.columns.values):\n",
    "            this_feature = {'name': feature_name}\n",
    "\n",
    "            if feature_name in numeric_features:\n",
    "                this_feature['isCont'] = True\n",
    "\n",
    "                this_feature['min'] = np.amin(features_np[:, i])\n",
    "                this_feature['max'] = np.amax(features_np[:, i])\n",
    "\n",
    "            else:\n",
    "                this_feature['isCont'] = False\n",
    "\n",
    "                this_feature['categories'] = features[feature_name].unique()\n",
    "\n",
    "            feature_information.append(this_feature)\n",
    "            \n",
    "        return feature_information\n",
    "    \n",
    "    @staticmethod\n",
    "    def _compare_data(extracted_samples, actual_samples, num_queries: None or int):\n",
    "        # convert data to numpy so that comparison becomes simpler\n",
    "        extracted_samples = extracted_samples.to_numpy().astype(float)\n",
    "        actual_samples = actual_samples.to_numpy().astype(float)\n",
    "        \n",
    "        # If only the features (without the labels) were extracted, then the labels are cut off from the actual_samples array\n",
    "        # in order to be able to compare the two arrays\n",
    "        if actual_samples.shape[1] > extracted_samples.shape[1]:\n",
    "            actual_samples = actual_samples[:,:-1]\n",
    "            \n",
    "        # drop duplicates from the extracted samples and from the actual samples to get accurate precision/recall\n",
    "        extracted_samples = np.unique(extracted_samples, axis=0)\n",
    "        actual_samples = np.unique(actual_samples, axis=0)\n",
    "        \n",
    "        # all_samples is the maximum amount of samples that could have been extracted during this attack\n",
    "        # If num_queries is None, it means the attack attempted to extracted all samples in the training data.\n",
    "        # Otherwise the attack stopped after num_queries queries.\n",
    "        if num_queries is None:\n",
    "            all_samples = len(actual_samples)\n",
    "        else:\n",
    "            all_samples = num_queries\n",
    "        \n",
    "        num_extracted_samples = extracted_samples.shape[0]\n",
    "        num_accurate_samples = 0\n",
    "        \n",
    "        for extracted_sample in extracted_samples:\n",
    "            logger.debug(f'Extracted sample: {extracted_sample}')\n",
    "\n",
    "            # Get all indices of the extracted sample in the given training data. features_np == row creates a boolean array \n",
    "            # with True if the cells match and False otherwise. all(axis=1) returns for each row if all elements in the row \n",
    "            # are True. np.where returns an array of indices where the boolean array contains the value True.\n",
    "            close_values = np.isclose(actual_samples, extracted_sample)\n",
    "            close_rows = close_values.all(axis=1)\n",
    "            indices_of_sample = np.where(close_rows)[0]\n",
    "\n",
    "            if indices_of_sample.shape[0] > 0:\n",
    "                logger.debug(f'Appears in training data at indices {indices_of_sample}')\n",
    "                num_accurate_samples += 1\n",
    "            else:\n",
    "                logger.debug('Does not appear in training data')\n",
    "        \n",
    "        return num_accurate_samples, num_extracted_samples, all_samples\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calc_precision_recall_tde(accurate_samples, num_extracted_samples, all_samples):\n",
    "        # Percentage of extracted samples that actually appears within the training data\n",
    "        if num_extracted_samples > 0:\n",
    "            precision = accurate_samples / num_extracted_samples\n",
    "        else:\n",
    "            # If the attack did not extract a single sample then precision cannot be calculated\n",
    "            precision = float(\"NaN\")\n",
    "\n",
    "        recall = accurate_samples / all_samples\n",
    "        \n",
    "        print(f'Number of extracted samples: {num_extracted_samples}')\n",
    "        print(f'Number of accurate extracted samples: {accurate_samples}')\n",
    "        print(f'Precision: {precision}, recall: {recall}')\n",
    "        \n",
    "        return precision, recall\n",
    "    \n",
    "    @staticmethod\n",
    "    def training_data_extraction_model_access(explainer, num_queries, feature_format, rng, model):\n",
    "        \"\"\"Executes training data extraction attack with access to the model\n",
    "        \n",
    "        Abstract method that must be implemented by subclass. Executes the attack against the explainer with access to the \n",
    "        model. Is allowed to make num_queries queries. If num_queries is None, makes as many queries as is necessary to\n",
    "        attempt to extract the full dataset. \n",
    "        Returns a dataframe containing all extracted samples.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        explainer\n",
    "            The explainer or explanation that will be attacked.\n",
    "        num_queries : None or int\n",
    "            The amount of queries to explainer allowed. If None, any number of queries is allowed.\n",
    "        feature_format\n",
    "            A dictionary that contains information for each sample (whether it is numeric or categorical, minimum, maximum,\n",
    "            the categories)\n",
    "        rng\n",
    "            Numpy rng object that can be used for reproducible random decisions.\n",
    "        model\n",
    "            The trained model that is explained by the explainer.\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        NotImplementedError\n",
    "            Must be implemented by subclass.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @staticmethod\n",
    "    def training_data_extraction_no_model_access(explainer, num_queries, feature_format, rng):\n",
    "        \"\"\"Executes training data extraction attack without access to the model\n",
    "        \n",
    "        Abstract method that must be implemented by subclass. Executes the attack against the explainer without access to the \n",
    "        model. Allowed to make num_queries queries to the explainer. If num_queries is None, then there is no limit. \n",
    "        Returns a dataframe containing all extracted samples.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        explainer\n",
    "            The explainer or explanation that will be attacked.\n",
    "        num_queries : None or int\n",
    "            The amount of queries to explainer allowed. If None, any number of queries is allowed.\n",
    "        feature_format\n",
    "            A dictionary that contains information for each sample (whether it is numeric or categorical, minimum, maximum,\n",
    "            the categories)\n",
    "        rng\n",
    "            Numpy rng object that can be used for reproducible random decisions.\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        NotImplementedError\n",
    "            Must be implemented by subclass.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019d0fa-8b16-4bb7-af66-8fb0eb3184c7",
   "metadata": {},
   "source": [
    "# Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d115c8a4-3c44-4be5-be7b-8bb91d21e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_experiments(experiment, dataset_dicts, model_dicts, random_state, num_queries, model_access, threads, results_table, is_mem_inf=True, convert_cat_to_str=False):\n",
    "    if type(num_queries) is dict:\n",
    "        num_queries_dict = num_queries\n",
    "    else:\n",
    "        num_queries_dict = {}\n",
    "        for dataset_dict in dataset_dicts:\n",
    "            num_queries_dict[dataset_dict['name']] = num_queries\n",
    "    \n",
    "    for dataset_dict in dataset_dicts:\n",
    "        # DiCE needs categorical features to be strings\n",
    "        if convert_cat_to_str:\n",
    "            cat_features = dataset_dict['dataset'].columns.difference(dataset_dict['num'] + [dataset_dict['outcome']])\n",
    "            for col in cat_features:\n",
    "                dataset_dict['dataset'][col] = dataset_dict['dataset'][col].astype(str)\n",
    "        \n",
    "        for model_dict in model_dicts:\n",
    "            dataset_name = dataset_dict['name']\n",
    "            model_name = model_dict['name']\n",
    "            print(f'dataset: {dataset_name}, model: {model_name}')\n",
    "            \n",
    "            EXP = experiment(dataset_dict['dataset'], dataset_dict['num'], dataset_dict['outcome'], random_state=random_state)\n",
    "            \n",
    "            if is_mem_inf:\n",
    "                accuracy, precision, recall = EXP.membership_inference_experiment(num_queries=num_queries_dict[dataset_name], model=model_dict['model'](random_state=random_state), model_access=model_access, threads=threads)\n",
    "                results_table.loc[len(results.index)] = [dataset_name, model_name, accuracy, precision, recall]\n",
    "            else:\n",
    "                precision, recall = EXP.training_data_extraction_experiment(num_queries=num_queries_dict[dataset_name], model=model_dict['model'](random_state=random_state), model_access=model_access, threads=threads)\n",
    "                results_table.loc[len(results.index)] = [dataset_name, model_name, precision, recall]\n",
    "                \n",
    "            with open(\"progress.txt\", mode='a') as file:\n",
    "                file.write(f'{datetime.datetime.now()}: Dataset {dataset_name}, model {model_name}, precision {precision}, recall {recall}.\\n')\n",
    "            \n",
    "    return results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434328a-8506-40a5-9eb0-cff09787e70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
